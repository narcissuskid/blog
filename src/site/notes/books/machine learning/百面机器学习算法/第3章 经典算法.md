---
{"dg-publish":true,"permalink":"/books/machine-learning//3/","tags":["百面机器学习"]}
---


# 经典算法

## 支持向量机SVM
- 硬间隔
   - 目标函数
      - 正负样本超平面间隔最大
      - 负样本超平面：
         令$\vec{w}=\frac{\vec{w'}}{\frac{b_{up}-b_{down}}{2}}$以及$b=\frac{b_{up}+b_{down}}{b_{up}-b_{down}}$
         $\begin{aligned}\vec{w'}\vec{x}+b_{up}=0\\\vec{w'}\vec{x}+\frac{b_{up}+b_{down}}{2}+\frac{b_{up}-b_{down}}{2}=0\\\vec{w}\vec{x}+b+1=0\end{aligned}$
      - 正样本超平面：$\begin{aligned}\vec{w'}\vec{x}+b_{down}=0\\\vec{w'}\vec{x}+\frac{b_{up}+b_{down}}{2}-\frac{b_{up}-b_{down}}{2}=0\\\vec{w}\vec{x}+b-1=0\end{aligned}$
      - 中间平面：$\vec{w}\vec{x}+b=0$
      - 样本在超平面的投影线性不可分
      - 间隔：$d=\frac{\left|b_{up}-b_{down}\right|}{\parallel{\vec{w'}}\parallel}=\frac{2}{\parallel{\vec{w}}\parallel}$
      - $\max{d}=\max{\frac{2}{\vec{w}}}\iff\min{\parallel{\vec{w}}\parallel}\iff\min{\frac{1}{2}\parallel{\vec{w}}\parallel^2}$
   - 约束条件
      - 负样本：$y_i=-1,\vec{w}\vec{x_i}+b+1<0$
      - 正样本：$y_i=1,\vec{w}\vec{xi}+b-1>0$
      - 合并后的约束条件为：$y_i(\vec{w}\vec{x_i}+b)\geq1$
   - 最终的数学表达
      $\min{f(\vec{w})}=\min{\frac{1}{2}\parallel{\vec{w}}\parallel^2}$
      $s.t. y_i(\vec{w}\vec{x_i}+b)\geq1$
   - 拉格朗日函数
      - $L(\vec{w},b,\alpha)=\frac{1}{2}\parallel{\vec{w}}\parallel^2-\sum\limits_{i=1}^N{\alpha_i[y_i(\vec{w}\vec{x_i}+b)-1]}$
      - 原始问题：$\min_{\vec{w},b}{\max_{\alpha}{L(\vec{w},b,\alpha)}}$
      - 对偶问题：$\max_{\alpha}{\min_{\vec{w},b}{L(\vec{w},b,\alpha)}}$
         - 求$\min_{\vec{w},b}{L(\vec{w},b,\alpha)}$
            - $\begin{aligned}\frac{\partial{L}}{\partial{\vec{w}}}=\vec{w}-\sum\limits_{i=1}^N{\alpha_iy_i\vec{x_i}}\end{aligned}=0$
            - $\frac{\partial{L}}{\partial{b}}=-\sum\limits_{i=1}^N{\alpha_iy_i}=0$
            - 带入$L(\vec{w},b,\alpha)$有$\begin{aligned}\min_{\vec{w},b}{L(\vec{w},b,\alpha)}&=\frac{1}{2}\sum\limits_{i=1}^N{\alpha_iy_i\vec{x_i}}\sum\limits_{j=1}^N{\alpha_jy_j\vec{x_j}}-\sum\limits_{i=1}^N{\alpha_iy_i\vec{x_i}}\sum\limits_{j=1}^N{\alpha_jy_j\vec{x_j}}+\sum\limits_{i=1}^N{\alpha_i}\\&=-\frac{1}{2}\sum\limits_{i=1}^N{\alpha_iy_i\vec{x_i}}\sum\limits_{j=1}^N{\alpha_jy_j\vec{x_j}}+\sum\limits_{i=1}^N{\alpha_i}\\&=-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N{\alpha_i\alpha_jy_iy_j\vec{x_i}\vec{x_j}}+\sum\limits_{i=1}^N{\alpha_i}\end{aligned}$
         - 求$\begin{aligned}\max_\alpha{\min_{\vec{w},b}{L(\vec{w},b,\alpha)}}\end{aligned}$
            - $\begin{aligned}\max_\alpha{\min_{\vec{w},b}{L(\vec{w},b,\alpha)}}&=\max_{\alpha}{-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N{\alpha_i\alpha_jy_iy_j\vec{x_i}\vec{x_j}}+\sum\limits_{i=1}^N\alpha_i}\\&=\min_\alpha{\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N{\alpha_i\alpha_jy_iy_j\vec{x_i}\vec{x_j}}-\sum\limits_{i=1}^N\alpha_i}\end{aligned}$
            - 约束条件：
               - $\sum\limits_{i=1}^N{\alpha_iy_i}=0$
               - $\alpha_i\ge0,i=1,2,\ldots,N$
      - slater条件
         - 原始问题的目标函数和约束条件($c_i(x)\le0,i=1,2,\ldots,k$)是凸函数
         - 原始问题的约束条件($h_i(x)=0,i=1,2,\ldots,l$)是仿射函数
         - 约束条件($c_i(x)\le0,i=1,2,\ldots,k$)严格满足(线性可分问题满足，即$y_i(\vec{w}\vec{x_i}+b)\gt1$)
         - 如果满足Slater条件，强对偶关系成立，对偶问题的解即是原始问题的解，即$\vec{\hat{w}}=\sum\limits_{i=1}^N{\hat{\alpha_i}y_i\vec{x_i}}$
      - KKT条件
         - 乘子非负：$\alpha_i\ge0,i=1,2,\ldots,N$
         - 约束条件：$y_i(\vec{w}\vec{x_i}+b)\geq1$
         - 互补条件：$\alpha_i(y_i(\vec{w}\vec{x_i}+b)-1)=0$
         - 至少存在一个$j$使$y_j(\vec{w}\vec{x_j}+\hat{b})=1$成立，即可得到$\hat{b}$
            - $\begin{aligned}\hat{b}&=\frac{1}{y_j}-\vec{\hat{w}}\vec{x_j}\\&=y_j-\sum\limits_{i=1}^N{\hat{\alpha_i}y_i\vec{x_i}\vec{x_j}}\end{aligned}$
   - 优化结果
      - 分离超平面：$\sum\limits_{i=1}^N{\hat{\alpha_i}y_i\vec{x_i}\vec{x}}+\hat{b}$
      - 决策函数：$f(x)=sign(\sum\limits_{i=1}^N{\hat{\alpha_i}y_i\vec{x_i}\vec{x}}+\hat{b})$
      - 支持向量：$\hat{\alpha_i}\not=0$
- 软间隔
   - 目标函数
      - 在硬间隔的基础上允许少量样本不满足约束$y_i(\vec{w}\vec{x_i}+b)\ge1$
      - 增加罚项后，目标函数变为$\min_{\vec{w},b}{\frac{1}{2}\parallel{\vec{w}}\parallel^2+C\sum\limits_{i=1}^N{\max{(0,1-y_i(\vec{w}\vec{x_i}+b))}}}$，$C$为罚项参数
         - $C$越小越容易欠拟合，$C$越大越容易过拟合
         - $C$取正无穷时，模型变为硬间隔
      - 引入松弛变量$\xi_i\ge0$后，目标函数变为
         - $\min_{\vec{w},b,\xi_i}\frac{1}{2}\parallel{\vec{w}}\parallel^2+C\xi_i$(**函数形式与带l2罚项的LR目标函数相似**)
         - $\begin{aligned}s.t. y_i(\vec{w}\vec{x_i}+b)\ge1-\xi_i\\\xi_i\ge0,i=1,2,\ldots,n\end{aligned}$
   - 对偶问题
      - 拉格朗日函数：$L(\vec{w},b,\xi,\alpha,\beta)=\frac{1}{2}\parallel{\vec{w}}\parallel^2+C\sum\limits_{i=1}^N{\xi_i}-\sum\limits_{i=1}^N{\alpha_i[y_i(\vec{w}\vec{x_i}+b)-1+\xi_i]-\sum\limits_{i=1}^N{\beta_i\xi_i}}$
      - 求$\min_{\vec{w},b,\xi}{L(\vec{w},b,\xi,\alpha,\beta)}$
         - $\begin{aligned}\frac{\partial{L}}{\partial{\vec{w}}}=\vec{w}-\sum\limits_{i=1}^N{\alpha_iy_i\vec{x_i}}\end{aligned}=0$
         - $\frac{\partial{L}}{\partial{b}}=-\sum\limits_{i=1}^N{\alpha_iy_i}=0$
         - $C=\alpha+\beta$
         - $\min_{\vec{w},b,\xi}{L(\vec{w},\xi,\alpha,\beta)}=-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N{\alpha_i\alpha_jy_iy_j\vec{x_i}\vec{x_j}}+\sum\limits_{i=1}^N{\alpha_i}$
      - 求$\max_{\alpha}\min_{\vec{w},b,\xi}L(\vec{w},\xi,\alpha,\beta)$
         - $\max_\alpha\min_{\vec{w},b,\xi}{L(\vec{w},\xi,\alpha,\beta)}=\min_\alpha{\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N{\alpha_i\alpha_jy_iy_j\vec{x_i}\vec{x_j}}-\sum\limits_{i=1}^N\alpha_i}$
         - 约束条件：
            - 乘子非负：$\alpha_i\ge0,\beta_i\ge0,i=1,2,\ldots,N$
            - 约束条件：$y_i(\vec{w}\vec{x_i}+b)-1+\xi_i\ge0$
            - 互补条件：$\alpha_i(y_i(\vec{w}\vec{x_i}+b)-1+\xi_i)=0,\beta_i\xi_i=0$
   - 求最优解
      - $\vec{\hat{w}}=\sum\limits_{i=1}^N{\hat{\alpha_i}y_i\vec{x_i}}$
      - $\hat{b}=y_j-\sum\limits_{i=1}^N{\hat{\alpha_i}y_i\vec{x_i}\vec{x_j}}$，$0\le\alpha_j\le{C}$
      - 间隔边界上：$\beta_i=0$
      - 间隔内：$0\lt\beta_i\lt1$
      - 间隔外：$\beta_i\gt1$
- 非线性SVM——核技巧
   - 使用一个变换将原空间的数据映射到新空间
   - 在新空间中使用线性方法训练模型
   - 核函数
      - $K(x,z)=\phi(x)\cdot\phi(z)$其中$\phi(x)\cdot\phi(z)$表示内积
      - 正定
      - 多项式核函数$K(x,z)=(x\cdot{z}+1)^p$
      - 高斯核$K(x,z)=e^{-\frac{\parallel{x-z}\parallel^2}{2\sigma^2}}$
- SMO算法
- 优点
   - 凸优化：一定是全局最优
   - 适用于线性与非线性问题
   - 高维数据也可以使用，因为模型复杂度主要取决于支持向量而不是数据维度
   - 理论基础完善
- 缺点
   - 不适合超大数据集
      - 涉及到m阶矩阵计算(m为样本个数)
      - SMO可以缓解
   - 只适用于二分类问题
      - 回归问题：推广到SVR
      - 多分类问题：多个SVM两两组合

## 逻辑回归
- 回归
   - 几率$odds=\frac{p}{1-p}$
   - 对于事件$y=1|x$的对数几率线性回归$\log\frac{p}{1-p}=\theta^Tx$
- vs 线性回归
   - 因变量$y$(而非$\frac{p}{1-p}$)是离散的，不是连续的
   - 广义线性模型
   - 都使用极大似然估计
   - 都可以使用梯度下降法

## 决策树
- 树的构造
   - NP完全问题
   - 采用启发式学习
      - ID3
         - 经验熵$H(D)=-\sum\limits_{k=1}^K{\frac{\left|C_k\right|}{\left|D\right|}\log_2\frac{\left|C_k\right|}{\left|D\right|}}$
            - $\left|D\right|$：样本元素个数
            - $\left|C_k\right|$：第$k$类样本个数
         - 条件熵$H(D|A)=\sum\limits_{i=1}^n\frac{\left|D_i\right|}{\left|D\right|}H(D_i)$
            - $D_i$：样本$D$中特征$A$取第$i$个值的样本子集
         - 信息增益$g(D,A)=H(D)-H(D|A)$
         - 缺点
            - 倾向取值较多的特征
            - 只能处理离散型变量
               - 理论上可以用类似C4.5的切分方法将连续特征转换为布尔型
               - 但ID3早于C4.5，当时没实现
            - 只能用于分类
            - 缺失值敏感
         - 优点
            - 每个结点可以产生多叉分支
      - C4.5
         - 信息增益比$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$
            - $H_A(D)=-\sum\limits_{i=1}^n\frac{\left|D_i\right|}{\left|D\right|}\log_2\frac{\left|D_i\right|}{\left|D\right|}$
         - 缺点
            - 只能用于分类
            - 新节点
         - 优点
            - 对缺失值有处理
            - 每个结点可以产生多叉分支
      - CART
         - 基尼指数——描述数据的纯度
            - $Gini(D)=1-\sum\limits_{k=1}^K(\frac{\left|C_k\right|}{\left|D\right|})^2$
            - $Gini(D|A)=\sum\limits_{i=1}^n\frac{\left|D_i\right|}{\left|D\right|}Gini(D_i)$
         - 优点
            - 分类和回归都可以使用（回归树使用MSE代替基尼指数）
         - 缺点
            - 只会产生两个分支
- 树的剪枝
   - 目的：防止过拟合
   - 方法
      - 预剪枝
         - 在生成决策树过程中提前停止树的增长
         - 新节点
      - 后剪枝
         - 在已生成的决策树上剪枝