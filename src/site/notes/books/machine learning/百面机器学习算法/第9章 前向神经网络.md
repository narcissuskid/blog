---
{"dg-publish":true,"permalink":"/books/machine-learning//9/","tags":["百面机器学习"]}
---


# 前向神经网络

## 网络模型
- 多层感知机
- 自编码器
- 限制波尔兹曼机
- 卷积神经网络

## 激活函数
- sigmoid
   - $f(z)=\frac{1}{1+e^{-z}}$
   - 导数$f^\prime(z)=f(z)(1-f(z))$
   - 梯度消失：当$z$很大时，$f(z)$趋于1，当$z$很小时，$f(z)$趋于0，导数$f^\prime(z)$趋于0
- tanh
   - $f(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}=2sigmoid(2z)-1$
   - 相当于sigmoid函数的平移
   - 导数$f^\prime(z)=1-(f(z))^2$
   - 梯度消失：当$z$很大时，$f(z)$趋于1，当$z$很小时，$f(z)$趋于-1，导数$f^\prime(z)$趋于0
- ReLU
   - $f(z)=\max(0,z)$
   - 导数$f^\prime(z)=\left\{\begin{aligned}&1,z\gt0;\\&0,z\le0\end{aligned}\right.$
   - 优点
      - 不需要计算指数
      - 解决梯度消失
      - 提供了稀疏表达能力
   - 缺点
      - 神经元死亡（梯度永远为0）
- Leaky ReLU
   - $f(z)=\left\{\begin{aligned}&z,z\gt0;\\&az,z\le0\end{aligned}\right.$
   - $a$为一个很小的正常数
   - 需要人工先验或多次重复训练确定合适的参数值
- PReLU
   - 将$a$变成一个可学习参数
   - 进行反向传播训练
- LReLU
   - 增加了随机化机制
- RReLU
   - 加入正则化

## 损失函数
- 平方误差
   - $\begin{aligned}J(W,b)&=[\frac{1}{m}\sum\limits_{i=1}^mJ(W,b;x^{(i)},y^{(i)})]+\frac{\lambda}{2}\sum\limits_{l=1}^{N-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1}}(W_{ij}^(l))^2\\&=[\frac{1}{m}\sum\limits_{i=1}^m\frac{1}{2}\parallel{y^{(i)}-L_{w,b}(x^{(i)})}\parallel^2]+\frac{\lambda}{2}\sum\limits_{l=1}^{N-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1}}(W_{ij}^{(l)})^2\end{aligned}$
   - $W_{ij}$参数梯度$\begin{aligned}\frac{\partial{J(W,b)}}{\partial{W_{ij}^{(l)}}}&=\frac{\partial{J(W,b)}}{\partial{z_j^{(l)}}}\frac{\partial{z_j^{(l)}}}{\partial{W_{ij}^{(l)}}}\\&=\delta_i^{(l)}x_j^{(l)}\end{aligned}$
   - $b_j$参数梯度$\begin{aligned}\frac{\partial{J(W,b)}}{\partial{b_{j}^{(l)}}}&=\frac{\partial{J(W,b)}}{\partial{z_j^{(l)}}}\frac{\partial{z_j^{(l)}}}{\partial{b_{j}^{(l)}}}\\&=\delta_i^{(l)}\end{aligned}$
   - 第$l$层第$i$个节点的残差量为$\delta_i^{(l)}=(\sum\limits_{j=1}^{s_{l+1}}W_{ij}^{(l+1)}\delta_j^{(l+1)})f^\prime(z_i^{(l)})$
   - 输出层残差$\delta^{(L)}=-(y-o^{(L)})f^\prime(z^{(L)})$
- 交叉熵损失
   - $\begin{aligned}J(W,b)&=[\frac{1}{m}\sum\limits_{i=1}^mJ(W,b;x^{(i)},y^{(i)})]+\frac{\lambda}{2}\sum\limits_{l=1}^{N-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1}}(W_{ij}^(l))^2\\&=-[\frac{1}{m}\sum\limits_{i=1}^m\left\{y^{(i)}\ln{o^{(i)}}+(1-y^{(i)})\ln{(1-o^{(i)})}\right\}]+\frac{\lambda}{2}\sum\limits_{l=1}^{N-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1}}(W_{ij}^{(l)})^2\end{aligned}$
   - 输出层残差$\delta^{(L)}=-\frac{f^\prime(z_k^{(L)})}{f(z_k^{(L)})}$
   - 当$f$为softmax函数时，$\delta^{(L)}=f(z_k^{(L)})-1$

## 过拟合
- 数据集增强
- 罚项
- 模型集成
- 批量归一化
   - 每批数据
   - 每层输入之前
   - 问题
      - 破坏了之前学到的特征分布
      - 解决
         - 引入变换重构和可学习参数方差$\gamma$和偏差$\beta$
         - $y^{(k)}=\gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}$
         - 与之前的网络层参数解耦
   - 卷积神经网络
      - 参数共享
- dropout
   - 按一定概率随机丢弃神经元节
   - 作用于每份小批量训练数据
   - 类似于bagging
   - 阶段
      - 训练
         - 前向传播
            - 每个神经元生成随机变量$r_j^{(l)}\backsim{Bernoulli}(p)$
            - 神经元输入$\widetilde{y}^{(l)}=r^{(l)}*y^{(l)}$
            - 神经元输出$y_i^{(l+1)}=f(w_i^{(l+1)}\widetilde{y}^{(l)}+b_i^{(l+1)})$
         - 反向传播
            - $r_j^{(l)}$取值为0的神经元不参与梯度和误差传播计算
      - 预测
         - 神经元的参数要乘以概率系数$p$

## 参数初始化
- 不能将同一层中神经元赋相同参数
   - 学习后的参数永远相等
- 可以初始化参数为取值范围$(-\frac{1}{\sqrt{d}},\frac{1}{\sqrt{d}})$的均匀分布，$d$为神经元接受的输入维度

## 卷积神经网络
- 稀疏交互
   - 卷积核尺度远小于输入维度
   - 局部特征结构
- 参数共享
   - 平移等变性
- 文本
   - 自动对N-gram特征进行组合和筛选

## 池化
- 目标
   - 非重叠区域
- 本质：降采样
- 方法
   - 均值
      - 能抑制由于邻域大小受限造成估计值方差增大的现象
      - 对背景保留效果更好
   - 最大
      - 能抑制网络参数误差造成估计值偏移现象
      - 更好地提取纹理信息
   - 相邻重叠区池化
   - 金字塔池化

## 深度
- 问题
   - 容易陷入局部最优解
   - 梯度消失问题更严重
- 改进
   - 网络结构
      - 深度残差网络(ResNet)
         - 缓解梯度消失
         - 两层网络叠加