---
{"dg-publish":true,"permalink":"/books/machine-learning//4/","tags":["百面机器学习"]}
---


# 降维

## 主成分分析
- 最大投影方差
   - 前提假设
      - 信号方差大
      - 噪声方差小
   - 参数说明
      - 原始数据点：$\left\{{\vec{v_1},\vec{v_2},\ldots,\vec{v_n}}\right\}$
      - 中心向量：$\vec{\mu}=\frac{1}{n}\sum\limits_{i=1}^n{\vec{v_i}}$
      - 中心化后：$\left\{\vec{x_1},\vec{x_2},\ldots,\vec{x_n}\right\}=\left\{\vec{v_1}-\vec{\mu},\vec{v_2}-\vec{\mu},\ldots,\vec{v_n}-\vec{\mu}\right\}$
      - 向量$\vec{x_i}$在单位向量$\vec{w}$上的投影坐标$(\vec{x_i},\vec{w})=\vec{x_i}^T\vec{w}$
         - 投影后的均值$\mu'=\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}^T\vec{w}}=(\frac{1}{n}\sum\limits_{i=1}^n\vec{x_i}^T)\vec{w}=0$
         - 投影后的方差$\begin{aligned}D(x)&=\frac{1}{n}\sum\limits_{i=1}^n(\vec{x_i}^T\vec{w})^2\\&=\frac{1}{n}\sum\limits_{i=1}^n(\vec{x_i}^T\vec{w})^T(\vec{x_i}^T\vec{w})\\&=\frac{1}{n}\sum\limits_{i=1}^n\vec{w}^T\vec{x_i}\vec{x_i}^T\vec{w}\\&=\vec{w}^T(\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}\vec{x_i}^T})\vec{w}\end{aligned}$其中$\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}\vec{x_i}^T}$是样本的协方差矩阵，简写为$\sum\limits$


## 线性判别分析
- 原理
   - 1.最大化类间距离
      - 类中心点
         - $\vec\mu_1=\frac{1}{N_1}\sum\limits_{\vec{x}\in{C1}}\vec{x}$
         - $\vec\mu_2=\frac{1}{N_2}\sum\limits_{\vec{x}\in{C2}}\vec{x}$
      - 类间距离$D(C_1,C_2)=\parallel\widetilde{\vec\mu_1}-\widetilde{\vec\mu_2}\parallel_2^2$
         - $\widetilde{\vec\mu_1}=\vec{w}^T\vec\mu_1$是$C_1$类中心在$\vec{w}$方向上的投影向量
         - $\widetilde{\vec\mu_2}=\vec{w}^T\vec\mu_2$是$C_2$类中心在$\vec{w}$方向上的投影向量
      - 目标优化问题
         - $\left\{\begin{aligned}\max_{\vec{w}}{\parallel\vec{w}^T(\mu_1-\mu_2)\parallel_2^2},\\s.t.\vec{w}^T\vec{w}=1\end{aligned}\right.$
   - 2.最小化类内距离
   - 3.兼顾类间和类内的目标优化问题
      - $\max_\vec{w}J(\vec{w})=\frac{\parallel\vec{w}^T(\mu_1-\mu_2)\parallel_2^2}{D_1+D_2}$
      - $C_1$类内距离用投影后的方差$D_1=\sum\limits_{\vec{x}\in{C_1}}(\vec{w}^T\vec{x}-\vec{w}^T\vec\mu_1)^2=\sum\limits_{\vec{x}\in{C_1}}\vec{w}^T(\vec{x}-\vec\mu_1)(\vec{x}-\vec\mu_1)^T\vec{w}$
      - $C_2$类内距离用投影后的方差$D_2=\sum\limits_{\vec{x}\in{C_2}}(\vec{w}^T\vec{x}-\vec{w}^T\vec\mu_2)^2=\sum\limits_{\vec{x}\in{C_2}}\vec{w}^T(\vec{x}-\vec\mu_2)(\vec{x}-\vec\mu_2)^T\vec{w}$
      - $\begin{aligned}J(\vec{w})&=\frac{\vec{w}^T(\vec\mu_1-\vec\mu_2)(\vec\mu_1-\vec\mu_2)^T\vec{w}}{\sum\limits_{\vec{x}\in{C_i}}\vec{w}^T(\vec{x}-\vec\mu_i)(\vec{x}-\vec\mu_i)^T\vec{w}}\\&=\frac{\vec{w}^TS_B\vec{w}}{\vec{w}^TS_w\vec{w}}\end{aligned}$
         - $\frac{\partial{J(\vec{w})}}{\partial{\vec{w}}}=\frac{(\frac{\partial{\vec{w}^TS_B\vec{w}}}{\partial{\vec{w}}}\vec{w}^TS_w\vec{w}-\frac{\partial{\vec{w}^TS_w\vec{w}}}{\partial{\vec{w}}}\vec{w}^TS_B\vec{w})}{(\vec{w}^TS_w\vec{w})^2}=0$
         - $(\vec{w}^TS_w\vec{w})S_B\vec{w}=(\vec{w}^TS_B\vec{w})S_w\vec{w}$
         - 令$\lambda=J(\vec{w})=\frac{\vec{w}^TS_B\vec{w}}{\vec{w}^TS_w\vec{w}}$有
         - $S_B\vec{w}=\lambda{S_w}\vec{w}$
         - $S_w^{-1}S_B\vec{w}=\lambda\vec{w}$
   - 4.最大化目标改为求解矩阵$S_w^{-1}S_B$的特征向量
- 二分类问题
   - 因为$S_B=(\vec\mu_1-\vec\mu_2)(\vec\mu_1-\vec\mu_2)^T$，所以$S_B\vec{w}$方向始终与$\mu_1-\mu_2$一致
   - 特征向量方向为$\vec{w}=S_w^{-1}(\mu_1-\mu_2)$
- 多分类问题
   - 全局散度矩阵
      - $S_t=\sum\limits_{i=1}^n(\vec{x_i}-\vec\mu)(\vec{x_i}-\vec\mu)^T$
   - 类间散度矩阵
      - $\begin{aligned}S_b&=S_t-S_w\\&=\sum\limits_{j=1}^n(\vec{x_i}-\vec\mu)(\vec{x_i}-\vec\mu)^T-\sum\limits_{\vec{x_i}\in{C_i}}(\vec{x}-\vec\mu)(\vec{x}-\vec\mu_i)^T\\&=\sum\limits_{j=1}^N(\sum\limits_{\vec{x}\in{C_j}}(\vec{x}-\vec\mu)(\vec{x}-\vec\mu)^T-\sum\limits_{\vec{x}\in{C_j}}(\vec{x}-\vec\mu_j)(\vec{x}-\vec\mu_j)^T)\\&=\sum\limits_{j=1}^Nm_j(\vec\mu_j-\vec\mu)(\vec\mu_j-\vec\mu)^T\end{aligned}$
- 特点
   - 有监督
- 优点
   - 比PCA更善于对有label的数据降维
- 缺点
   - 对数据分布做了强假设
      - 每个类数据都是高斯分布
      - 每个类的协方差相等
   - 但可以通过引入核函数扩展LDA，处理分布更复杂的问题

## 流形映射
- 等距映射
- 局部线性嵌入
- 拉普拉斯特征映射
- 局部保留投影