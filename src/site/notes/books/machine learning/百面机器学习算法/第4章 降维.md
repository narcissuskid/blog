---
{"dg-publish":true,"permalink":"/books/machine-learning//4/","tags":["百面机器学习"]}
---


# 降维

## 主成分分析
- 最大投影方差
   - 前提假设
      - 信号方差大
      - 噪声方差小
   - 参数说明
      - 原始数据点：$\left\{{\vec{v_1},\vec{v_2},\ldots,\vec{v_n}}\right\}$
      - 中心向量：$\vec{\mu}=\frac{1}{n}\sum\limits_{i=1}^n{\vec{v_i}}$
      - 中心化后：$\left\{\vec{x_1},\vec{x_2},\ldots,\vec{x_n}\right\}=\left\{\vec{v_1}-\vec{\mu},\vec{v_2}-\vec{\mu},\ldots,\vec{v_n}-\vec{\mu}\right\}$
      - 向量$\vec{x_i}$在单位向量$\vec{w}$上的投影坐标$(\vec{x_i},\vec{w})=\vec{x_i}^T\vec{w}$
         - 投影后的均值$\mu'=\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}^T\vec{w}}=(\frac{1}{n}\sum\limits_{i=1}^n\vec{x_i}^T)\vec{w}=0$
         - 投影后的方差$\begin{aligned}D(x)&=\frac{1}{n}\sum\limits_{i=1}^n(\vec{x_i}^T\vec{w})^2\\&=\frac{1}{n}\sum\limits_{i=1}^n(\vec{x_i}^T\vec{w})^T(\vec{x_i}^T\vec{w})\\&=\frac{1}{n}\sum\limits_{i=1}^n\vec{w}^T\vec{x_i}\vec{x_i}^T\vec{w}\\&=\vec{w}^T(\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}\vec{x_i}^T})\vec{w}\end{aligned}$其中$\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}\vec{x_i}^T}$是样本的协方差矩阵，简写为$\sum\limits$
   - 最优化目标
      - $\left\{\begin{aligned}\max\left\{\vec{w}^T\sum\limits\vec{w}\right\},\\s.t.\vec{w}^T\vec{w}=1\end{aligned}\right\}$
      - 拉格朗日函数$L(\vec{w},\lambda)=\vec{w}^T\sum\limits\vec{w}-\lambda(\vec{w}^T\vec{w}-1)$
         - 对$\vec{w}$求导$\frac{\partial{L(\vec{w},\lambda)}}{\partial\vec{w}}=\sum\limits\vec{w}-\lambda\vec{w}=0$
         - $\max{D(x)}=\lambda\vec{w}^T\vec{w}=\lambda$
         - 由$\sum\limits\vec{w}=\lambda\vec{w}$可得$\lambda$是协方差矩阵$\sum\limits$的特征值
   - 求解步骤
      - 1.对样本进行中心化处理
      - 2.求样本协方差矩阵
      - 3.对协方差矩阵进行特征值分解
      - 4.取特征值前$d$大对应的特征向量$\vec{w_1},\vec{w_2},\ldots,\vec{w_d}$
      - 5.将$n$维样本映射到$d$维$\vec{x_i'}=\left[\begin{aligned}\vec{w_1}^T\vec{x_i}\\\vec{w_2}^T\vec{x_i}\\\vdots\\\vec{w_d}^T\vec{x_i}\end{aligned}\right]$
      - 6.降维后的信息占比为$\eta=\sqrt{\frac{\sum\limits_{i=1}^d\lambda_i^2}{\sum\limits_{i=1}^n\lambda_i^2}}$
   - 特点
      - 线性
         - 可以通过核映射扩展得到核主成分分析(KPCA)
      - 非监督
      - 全局最优
- 最小平方误差
   - 参数说明
      - 找到距离平方和最小的$d$维超平面，标准正交基为$W=\left\{\vec{w_1},\vec{w_2},\cdots,\vec{w_d}\right\}$
      - $distance(\vec{x_k},D)=\parallel{\vec{x_k}-\widetilde{\vec{x_k}}}\parallel_2$其中$\widetilde{\vec{x_k}}$表示$\vec{x_k}$在$D$上的投影向量
      - $\widetilde{\vec{x_k}}=\sum\limits_{i=1}^d(\vec{w_i}^T\vec{x_k})\vec{w_i}$，其中$\vec{w_i}^T\vec{x_k}$是$\vec{x_k}$在$\vec{w_i}$方向上的投影长度
   - 最优化目标
      - $\left\{\begin{aligned}\min_{\vec{w_i},\cdots,\vec{w_d}}\sum\limits_{k=1}^n\parallel{\vec{x_k}-\widetilde{\vec{x_k}}}\parallel_2^2,\\s.t.\vec{w_i}^T\vec{w_j}=\delta_{ij}=\left\{\begin{aligned}1,i=j;\\0,i\not=j.\end{aligned}\right.\end{aligned}\right.$
      - $\begin{aligned}\parallel\vec{x_k}-\widetilde{\vec{x_k}}\parallel_2^2&=(\vec{x_k}-\widetilde{\vec{x_k}})^T(\vec{x_k}-\widetilde{\vec{x_k}})\\&=\vec{x_k}^T\vec{x_k}-\vec{x_k}^T\widetilde{\vec{x_k}}-\widetilde{\vec{x_k}}^T\vec{x_k}+\widetilde{\vec{x_k}}^T\widetilde{\vec{x_k}}\\&=\vec{x_k}^T\vec{x_k}-2\vec{x_k}^T\widetilde{\vec{x_k}}+\widetilde{\vec{x_k}}^T\widetilde{\vec{x_k}}\end{aligned}$
      - $\begin{aligned}\vec{x_k}\widetilde{\vec{x_k}}&=\vec{x_k}^T\sum\limits_{i=1}^d{(\vec{w_i}^T\vec{x_k})\vec{w_i}}\\&=\sum\limits_{i=1}^d(\vec{w_i}^T\vec{x_k})\vec{x_k}^T\vec{w_i}\\&=\sum\limits_{i=1}^d{\vec{w_i}^T\vec{x_k}\vec{x_k}^T\vec{w_i}}\end{aligned}$
      - $\begin{aligned}\widetilde{\vec{x_k}}^T\widetilde{\vec{x_k}}&=(\sum\limits_{i=1}^d(\vec{w_i}^T\vec{x_k})\vec{w_i})^T(\sum\limits_{j=1}^d(\vec{w_j}^T\vec{x_k})\vec{w_j})\\&=\sum\limits_{i=1}^d(\vec{w_i}^T\vec{x_k})\vec{w_i}^T\vec{w_i}(\vec{x_k}^T\vec{w_i})\\&=\sum\limits_{i=1}^d{\vec{w_i}^T\vec{x_k}\vec{x_k}^T\vec{w_i}}\end{aligned}$
      - $\begin{aligned}\parallel\vec{x_k}-\widetilde{\vec{x_k}}\parallel_2^2&=-\sum\limits_{i=1}^d\vec{w_i}^T\vec{x_k}\vec{x_k}^T\vec{w_i}+\vec{x_k}^T\vec{x_k}\\&=-tr(W^T\vec{x_k}\vec{x_k}^TW)+\vec{x_k}^T\vec{x_k}\end{aligned}$其中$tr()$表示求矩阵的迹(对角线元素之和)
      - $\begin{aligned}\min\sum\limits_{k=1}^n\parallel{\vec{x_k}-\widetilde{\vec{x_k}}}\parallel_2^2&=\sum\limits_{k=1}^n(-tr(W^T\vec{x_k}\vec{x_k}^TW)+\vec{x_k}^T\vec{x_k})\\&=-\sum\limits_{k=1}^ntr(W^T\vec{x_k}\vec{x_k}^TW)+c\\&=-tr(W^TXX^TW)+C\end{aligned}$
      - 目标函数变为$\left\{\begin{aligned}\max_Wtr(W^TXX^TW),\\s.t.W^TW=I\end{aligned}\right.$
      - 当对$W$中$d$个基依次求解时，和最大方差理论方法完全等价

## 流形映射
- 等距映射
- 局部线性嵌入
- 拉普拉斯特征映射
- 局部保留投影

## 线性判别分析
- 原理