---
{"dg-publish":true,"permalink":"/books/machine-learning//4/","tags":["百面机器学习"]}
---


# 降维

## 主成分分析
- 原理
   - 最大投影方差
      - 信号具有较大方差
      - 噪声具有较小方差
      - 先对所有向量中心化
         - 原始数据点：$\left\{{\vec{v_1},\vec{v_2},\ldots,\vec{v_n}}\right\}$
         - 中心向量：$vec{\mu}=\frac{1}{n}\sum\limits_{i=1}^n{\vec{v_i}}$
         - 中心化后：$\left\{\vec{x_1},\vec{x_2},\ldots,\vec{x_n}\right\}=\left\{\vec{v_1}-\vec{\mu},\vec{v_2}-\vec{\mu},\ldots,\vec{v_n}-\vec{\mu}\right\}$
      - 向量$\vec{x_i}$在单位向量$\vec{w}$上的投影坐标$(\vec{x_i},\vec{w})=\vec{x_i}^T\vec{w}$
         - 投影后的均值$\mu'=\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}^T\vec{w}}=(\frac{1}{n}\sum\limits_{i=1}^n\vec{x_i}^T)\vec{w}=0$
         - 投影后的方差$\begin{aligned}D(x)&=\frac{1}{n}\sum\limits_{i=1}^n(\vec{x_i}^T\vec{w})^2\\&=\frac{1}{n}\sum\limits_{i=1}^n(\vec{x_i}^T\vec{w})^T(\vec{x_i}^T\vec{w})\\&=\frac{1}{n}\sum\limits_{i=1}^n\vec{w}^T\vec{x_i}\vec{x_i}^T\vec{w}\\&=\vec{w}^T(\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}\vec{x_i}^T})\vec{w}\end{aligned}$其中$\frac{1}{n}\sum\limits_{i=1}^n{\vec{x_i}\vec{x_i}^T}$是样本的协方差矩阵，简写为$\sum\limits$
      - 最优化目标
         - $\left\{\begin{aligned}\max\left\{\vec{w}^T\sum\limits\vec{w}\right\},\\s.t.\vec{w}^T\vec{w}=1\end{aligned}\right\}$
         - 拉格朗日函数$L(\vec{w},\lambda)=\vec{w}^T\sum\limits\vec{w}-\lambda(\vec{w}^T\vec{w}-1)$
            - 对$\vec{w}$求导$\frac{\partial{L(\vec{w},\lambda)}}{\partial\vec{w}}=\sum\limits\vec{w}-\lambda\vec{w}=0$
            - $\max{D(x)}=\lambda\vec{w}^T\vec{w}=\lambda$
            - 由$\sum\limits\vec{w}=\lambda\vec{w}$可得$\lambda$是协方差矩阵$\sum\limits$的特征值

## 流形映射
- 等距映射
- 局部线性嵌入
- 拉普拉斯特征映射
- 局部保留投影