---
{"dg-publish":true,"permalink":"/books/machine-learning//4/","tags":["百面机器学习"]}
---


# 降维

## 主成分分析
- 最大投影方差
   - 前提假设
      - 信号方差大
      - 噪声方差小
   - 参数说明
      - 原始数据点：$\left\{{\vec{v_1},\vec{v_2},\ldots,\vec{v_n}}\right\}$
      - 中心向量：$\vec{\mu}=\frac{1}{n}\sum\limits_{i=1}^n{\vec{v_i}}$
      - 
   - 最优化目标
      - $\left\{\begin{aligned}\max\left\{\vec{w}^T\sum\limits\vec{w}\right\},\\s.t.\vec{w}^T\vec{w}=1\end{aligned}\right\}$
      - 拉格朗日函数$L(\vec{w},\lambda)=\vec{w}^T\sum\limits\vec{w}-\lambda(\vec{w}^T\vec{w}-1)$
         - 对$\vec{w}$求导$\frac{\partial{L(\vec{w},\lambda)}}{\partial\vec{w}}=\sum\limits\vec{w}-\lambda\vec{w}=0$
         - $\max{D(x)}=\lambda\vec{w}^T\vec{w}=\lambda$
         - 由$\sum\limits\vec{w}=\lambda\vec{w}$可得$\lambda$是协方差矩阵$\sum\limits$的特征值
   - 求解步骤
      - 1.对样本进行中心化处理
      - 2.求样本协方差矩阵
      - 3.对协方差矩阵进行特征值分解
      - 4.取特征值前$d$大对应的特征向量$\vec{w_1},\vec{w_2},\ldots,\vec{w_d}$
      - 5.将$n$维样本映射到$d$维$\vec{x_i'}=\left[\begin{aligned}\vec{w_1}^T\vec{x_i}\\\vec{w_2}^T\vec{x_i}\\\vdots\\\vec{w_d}^T\vec{x_i}\end{aligned}\right]$
      - 6.降维后的信息占比为$\eta=\sqrt{\frac{\sum\limits_{i=1}^d\lambda_i^2}{\sum\limits_{i=1}^n\lambda_i^2}}$
   - 特点
      - 线性
         - 可以通过核映射扩展得到核主成分分析(KPCA)
      - 非监督
      - 全局最优
- 最小平方误差
   - 参数说明
      - 找到距离平方和最小的$d$维超平面，标准正交基为$W=\left\{\vec{w_1},\vec{w_2},\cdots,\vec{w_d}\right\}$
      - $distance(\vec{x_k},D)=\parallel{\vec{x_k}-\widetilde{\vec{x_k}}}\parallel_2$其中$\widetilde{\vec{x_k}}$表示$\vec{x_k}$在$D$上的投影向量
      - $\widetilde{\vec{x_k}}=\sum\limits_{i=1}^d(\vec{w_i}^T\vec{x_k})\vec{w_i}$，其中$\vec{w_i}^T\vec{x_k}$是$\vec{x_k}$在$\vec{w_i}$方向上的投影长度
   - 最优化目标
      - $\left\{\begin{aligned}\min_{\vec{w_i},\cdots,\vec{w_d}}\sum\limits_{k=1}^n\parallel{\vec{x_k}-\widetilde{\vec{x_k}}}\parallel_2^2,\\s.t.\vec{w_i}^T\vec{w_j}=\delta_{ij}=\left\{\begin{aligned}1,i=j;\\0,i\not=j.\end{aligned}\right.\end{aligned}\right.$
      - $\begin{aligned}\parallel\vec{x_k}-\widetilde{\vec{x_k}}\parallel_2^2&=(\vec{x_k}-\widetilde{\vec{x_k}})^T(\vec{x_k}-\widetilde{\vec{x_k}})\\&=\vec{x_k}^T\vec{x_k}-\vec{x_k}^T\widetilde{\vec{x_k}}-\widetilde{\vec{x_k}}^T\vec{x_k}+\widetilde{\vec{x_k}}^T\widetilde{\vec{x_k}}\\&=\vec{x_k}^T\vec{x_k}-2\vec{x_k}^T\widetilde{\vec{x_k}}+\widetilde{\vec{x_k}}^T\widetilde{\vec{x_k}}\end{aligned}$
      - $\begin{aligned}\vec{x_k}\widetilde{\vec{x_k}}&=\vec{x_k}^T\sum\limits_{i=1}^d{(\vec{w_i}^T\vec{x_k})\vec{w_i}}\\&=\sum\limits_{i=1}^d(\vec{w_i}^T\vec{x_k})\vec{x_k}^T\vec{w_i}\\&=\sum\limits_{i=1}^d{\vec{w_i}^T\vec{x_k}\vec{x_k}^T\vec{w_i}}\end{aligned}$
      - $\begin{aligned}\widetilde{\vec{x_k}}^T\widetilde{\vec{x_k}}&=(\sum\limits_{i=1}^d(\vec{w_i}^T\vec{x_k})\vec{w_i})^T(\sum\limits_{j=1}^d(\vec{w_j}^T\vec{x_k})\vec{w_j})\\&=\sum\limits_{i=1}^d(\vec{w_i}^T\vec{x_k})\vec{w_i}^T\vec{w_i}(\vec{x_k}^T\vec{w_i})\\&=\sum\limits_{i=1}^d{\vec{w_i}^T\vec{x_k}\vec{x_k}^T\vec{w_i}}\end{aligned}$
      - $\begin{aligned}\parallel\vec{x_k}-\widetilde{\vec{x_k}}\parallel_2^2&=-\sum\limits_{i=1}^d\vec{w_i}^T\vec{x_k}\vec{x_k}^T\vec{w_i}+\vec{x_k}^T\vec{x_k}\\&=-tr(W^T\vec{x_k}\vec{x_k}^TW)+\vec{x_k}^T\vec{x_k}\end{aligned}$其中$tr()$表示求矩阵的迹(对角线元素之和)
      - $\begin{aligned}\min\sum\limits_{k=1}^n\parallel{\vec{x_k}-\widetilde{\vec{x_k}}}\parallel_2^2&=\sum\limits_{k=1}^n(-tr(W^T\vec{x_k}\vec{x_k}^TW)+\vec{x_k}^T\vec{x_k})\\&=-\sum\limits_{k=1}^ntr(W^T\vec{x_k}\vec{x_k}^TW)+c\\&=-tr(W^TXX^TW)+C\end{aligned}$
      - 目标函数变为$\left\{\begin{aligned}\max_Wtr(W^TXX^TW),\\s.t.W^TW=I\end{aligned}\right.$
      - 当对$W$中$d$个基依次求解时，和最大方差理论方法完全等价

## 线性判别分析
- 原理
   - 1.最大化类间距离
      - 类中心点
         - $\vec\mu_1=\frac{1}{N_1}\sum\limits_{\vec{x}\in{C1}}\vec{x}$
         - $\vec\mu_2=\frac{1}{N_2}\sum\limits_{\vec{x}\in{C2}}\vec{x}$
      - 类间距离$D(C_1,C_2)=\parallel\widetilde{\vec\mu_1}-\widetilde{\vec\mu_2}\parallel_2^2$
         - $\widetilde{\vec\mu_1}=\vec{w}^T\vec\mu_1$是$C_1$类中心在$\vec{w}$方向上的投影向量
         - $\widetilde{\vec\mu_2}=\vec{w}^T\vec\mu_2$是$C_2$类中心在$\vec{w}$方向上的投影向量
      - 目标优化问题
         - $\left\{\begin{aligned}\max_{\vec{w}}{\parallel\vec{w}^T(\mu_1-\mu_2)\parallel_2^2},\\s.t.\vec{w}^T\vec{w}=1\end{aligned}\right.$
   - 2.最小化类内距离
   - 3.兼顾类间和类内的目标优化问题
      - $\max_\vec{w}J(\vec{w})=\frac{\parallel\vec{w}^T(\mu_1-\mu_2)\parallel_2^2}{D_1+D_2}$
      - $C_1$类内距离用投影后的方差$D_1=\sum\limits_{\vec{x}\in{C_1}}(\vec{w}^T\vec{x}-\vec{w}^T\vec\mu_1)^2=\sum\limits_{\vec{x}\in{C_1}}\vec{w}^T(\vec{x}-\vec\mu_1)(\vec{x}-\vec\mu_1)^T\vec{w}$
      - $C_2$类内距离用投影后的方差$D_2=\sum\limits_{\vec{x}\in{C_2}}(\vec{w}^T\vec{x}-\vec{w}^T\vec\mu_2)^2=\sum\limits_{\vec{x}\in{C_2}}\vec{w}^T(\vec{x}-\vec\mu_2)(\vec{x}-\vec\mu_2)^T\vec{w}$
      - $\begin{aligned}J(\vec{w})&=\frac{\vec{w}^T(\vec\mu_1-\vec\mu_2)(\vec\mu_1-\vec\mu_2)^T\vec{w}}{\sum\limits_{\vec{x}\in{C_i}}\vec{w}^T(\vec{x}-\vec\mu_i)(\vec{x}-\vec\mu_i)^T\vec{w}}\\&=\frac{\vec{w}^TS_B\vec{w}}{\vec{w}^TS_w\vec{w}}\end{aligned}$
         - $\frac{\partial{J(\vec{w})}}{\partial{\vec{w}}}=\frac{(\frac{\partial{\vec{w}^TS_B\vec{w}}}{\partial{\vec{w}}}\vec{w}^TS_w\vec{w}-\frac{\partial{\vec{w}^TS_w\vec{w}}}{\partial{\vec{w}}}\vec{w}^TS_B\vec{w})}{(\vec{w}^TS_w\vec{w})^2}=0$
         - $(\vec{w}^TS_w\vec{w})S_B\vec{w}=(\vec{w}^TS_B\vec{w})S_w\vec{w}$
         - 令$\lambda=J(\vec{w})=\frac{\vec{w}^TS_B\vec{w}}{\vec{w}^TS_w\vec{w}}$有
         - $S_B\vec{w}=\lambda{S_w}\vec{w}$
         - $S_w^{-1}S_B\vec{w}=\lambda\vec{w}$
   - 4.最大化目标改为求解矩阵$S_w^{-1}S_B$的特征向量
- 二分类问题
   - 因为$S_B=(\vec\mu_1-\vec\mu_2)(\vec\mu_1-\vec\mu_2)^T$，所以$S_B\vec{w}$方向始终与$\mu_1-\mu_2$一致
   - 特征向量方向为$\vec{w}=S_w^{-1}(\mu_1-\mu_2)$
- 多分类问题
   - 全局散度矩阵
      - $S_t=\sum\limits_{i=1}^n(\vec{x_i}-\vec\mu)(\vec{x_i}-\vec\mu)^T$
   - 类间散度矩阵
      - $\begin{aligned}S_b&=S_t-S_w\\&=\sum\limits_{j=1}^n(\vec{x_i}-\vec\mu)(\vec{x_i}-\vec\mu)^T-\sum\limits_{\vec{x_i}\in{C_i}}(\vec{x}-\vec\mu)(\vec{x}-\vec\mu_i)^T\\&=\sum\limits_{j=1}^N(\sum\limits_{\vec{x}\in{C_j}}(\vec{x}-\vec\mu)(\vec{x}-\vec\mu)^T-\sum\limits_{\vec{x}\in{C_j}}(\vec{x}-\vec\mu_j)(\vec{x}-\vec\mu_j)^T)\\&=\sum\limits_{j=1}^Nm_j(\vec\mu_j-\vec\mu)(\vec\mu_j-\vec\mu)^T\end{aligned}$
- 特点
   - 有监督
- 优点
   - 比PCA更善于对有label的数据降维
- 缺点
   - 对数据分布做了强假设
      - 每个类数据都是高斯分布
      - 每个类的协方差相等
   - 但可以通过引入核函数扩展LDA，处理分布更复杂的问题

## 流形映射
- 等距映射
- 局部线性嵌入
- 拉普拉斯特征映射
- 局部保留投影