---
{"dg-publish":true,"permalink":"/books/machine-learning//5/","tags":["百面机器学习"]}
---


# 非监督学习

## 特征变量关联—相关性分析

## 数据聚类—迭代最优
- 算法
   - K-means
      - 本质：基于距离度量的数据划分
         - 欧式距离
            - （高纬）球形分布
         - 核函数
            - 非凸数据分布
      - 代价函数$J(c,\mu)=\sum\limits_{i=1}^M\parallel{\vec{x_i}-\vec\mu_{c_{i}}}\parallel^2$
      - 步骤
         - 数据预处理
            - 归一化
            - 离群点
         - 随机选取K个中心点$\vec\mu_1^0,\vec\mu_2^0,\vec\cdots,\vec\mu_k^0$
         - 代价函数$J(c,\mu)=\min_\mu\min_c\sum\limits_{i=1}^M\parallel{\vec{x_i}-\vec\mu_{c_i}}\parallel^2$
         - 迭代t次直至$J$收敛
            - 将每个样本$\vec{x_i}$分配到距离最近的类
            - 重新计算类的中心
      - 调优
         - 数据预处理（归一和离群点）
         - 合理选择K值
            - [[books/machine learning/百面机器学习算法/annotation#^3k6slxpanxc\|手肘法]]
            - Gap Statistic
               - 可以自动化
               - $Gap(K)=E(\log{D_k})-\log{D_k}$
               - $E(\log{D_k})$是期望，有蒙特卡洛模拟产生
               - 在样本区域内按均匀分布随机产生和原始样本数一样多的随机样本得到一个$D_k$
         - 采用核函数
      - 优点
         - 可伸缩
         - 高效（计算复杂度接近线性）
      - 缺点
         - 结果不稳定
         - 局部而非全局最优
         - 不适合类分布差别加大的情况
         - 不适合离散分类
         - 需要人工预先设定K值，可能和真实数据不一致
         - 样本只能被划分到单一类中
      - 迭代
   - K-means++
      - 优化初始K个聚类中心
      - 选取第n+1个点时，离当前n个聚类中心越远的点有更高的概率被选到
   - ISODATA算法
      - 自动迭代K值
         - 分裂：增加K
         - 合并：减少KK
      - 缺点
         - 超参数多
            - 预期$K_0$
            - 每个类的最少样本数$N_{min}$
            - 分类阈值：类内最大方差
            - 合并阈值：类间最小距离
   - EM算法
      - 通用框架
         - 1.先固定一个变量使目标函数变为凸优化函数
         - 2.求导得到最值
         - 3.利用最优参数更新固定变量
         - 4.返回步骤一直至收敛
      - 目标：解决在概率模型中含有无法观测的隐变量情况下的参数估计问题
         - 最大似然函数$\theta=\underset{\theta}{\operatorname{arg\,max}}\sum\limits_{i=1}^m\log{P(x_i|\theta)}$
         - 当存在隐变量时$\theta=\underset{\theta}{\operatorname{arg\,max}}\sum\limits_{i=1}^m\log\sum\limits_{z_i}P(x_i,z_i|\theta)$
      - 假设隐变量$z_i$对应的分布为$Q_i(z_i)$
         - $\sum\limits_{z_i}Q_i(z_i)=1$
         - 由Jensen不等式有$\begin{aligned}\sum\limits_{i=1}^m\log\sum\limits_{z_i}P(x_i,z_i|\theta)&=\sum\limits_{i=1}^m\log\sum\limits_{z_i}Q_i(z_i)\frac{P(x_i,z_i|\theta)}{Q_i(z_i)}\\&\ge\sum\limits_{i=1}^m\sum\limits_{z_i}Q_i(z_i)\log\frac{P(x_i,z_i|\theta)}{Q_i(z_i)}\end{aligned}$
         - 当且仅当$\frac{P(x_i,z_i|\theta)}{Q_i(z_i)}=c$等式成立，其中$c$为常数
            - 此时满足$\sum\limits_{i=1}^m\log\sum\limits_{z_i}P(x_i,z_i|\theta)=\sum\limits_{i=1}^m\log\frac{P(x_i,z_i|\theta)}{Q_i(z_i)}$
            - 所以$Q_i(z_i)=\frac{P(x_i,z_i|\theta)}{\sum\limits_{z_i}P(x_i,z_i|\theta)}=P(z_i|x_i,\theta)$
      - 通过最大化下界改进待优化函数
         - $\begin{aligned}B(\theta_t+1,\theta_t)&=L(\theta_{t+1})-L(\theta_t)\\&=\log{P(x|\theta_{t+1})}-\log{P(x|\theta_t)}\\&=\log{\sum\limits_zP(x,z|\theta_{t+1})}-\log{P(x|\theta_t)}\\&=\log{\sum\limits_zP(z|x,\theta_t)\frac{P(x,z|\theta_{t+1})}{P(z|x,\theta_t)}}-\sum\limits_zP(z|x,\theta_i)\log{P(x|\theta_t)}\\&\ge\sum\limits_zP(z|x,\theta_t)\log\frac{P(x,z|\theta_{t+1})}{P(z|x,\theta_t)}-\sum\limits_zP(z|x,\theta_i)\log{P(x|\theta_t)}\\&=\sum\limits_zP(z|x,\theta_t)\log\frac{P(x,z|\theta_{t+1})}{P(z|x,\theta_t)P(x|\theta_t)}\\&=\sum\limits_zP(z|x,\theta_t)\log\frac{P(x,z|\theta_{t+1})}{P(x,z|\theta_t)}\\&\ge0\end{aligned}$
         - 因为每次迭代的目的是使$P(x,z|\theta_{t+1})\ge{P(x,z|\theta_t)}$，所以$L(\theta_{t+1})\ge{L(\theta_t)}$
         - 最大化目标为$\begin{aligned}\operatorname{arg\,max}L(\theta_{i+1})&=\operatorname{arg\,max}B(\theta_{t+1},\theta_t)+L(\theta_t)\\&\iff\operatorname{arg\,max}\sum\limits_zP(z|x,\theta_t)\log{P(x,z|\theta_{t+1})}\\&\iff\operatorname{arg\,max}{E_{z|x,\theta_t}[\log{P(x,z|\theta_{t+1})}}]\end{aligned}$
      - 收敛性证明
         - $\begin{aligned}L(\theta)&=\log{P(x|\theta)}\\&=\log{\frac{P(x,z|\theta)}{P(z|x,\theta)}}\\&=\log{P(x,z|\theta}-\log{P(z|x,\theta)}\\&=\sum\limits_z{P(z|x,\theta_t)}\log{P(x,z|\theta)}-\sum\limits_z{P(z|x,\theta_t)\log{P(z|x,\theta)}}\\&=Q(\theta,\theta_t)-H(\theta,\theta_t)\end{aligned}$
         - 因为$\begin{aligned}H(\theta,\theta_{t+1})-H(\theta,\theta_t)&=\sum\limits_zP(z|x,\theta_t)\log\frac{P(z|x,\theta_{t+1})}{P(z|x,\theta_t)}\\&\le\sum\limits_z\log{P(z|x,\theta_t)}\times\frac{P(z|x,\theta_{t+1})}{P(z|x,\theta_t)}\\&=\sum\limits_z\log{P(z|x,\theta_{t+1})}\\&\le0\end{aligned}$
         - 又因为$Q(\theta_{t+1},\theta_t)\ge{Q(\theta_t,\theta_t)}$
         - 所以$L(\theta_{t+1})\ge{L(\theta_t)}$
      - K-means算法等价于用EM求解含有$z\in\left\{1,2,\cdots,k\right\}$隐变量的最大似然问题
         - $P(x,z|\mu_1,\mu_2,\cdots,\mu_k)\propto\left\{\begin{aligned}&\exp(-\parallel{x-\mu_z}\parallel_2^2),\parallel{x-\mu_2}\parallel_2=\min_k\parallel{x-\mu_k}\parallel_2\\&0,\parallel{x-\mu_z}\parallel_2\gt\min_k\parallel{x-\mu_k}\parallel_2\end{aligned}\right.$
         - E步骤对每个点找到当前最近簇$Q(z)=P(z|x,\mu_1,\mu_2,\cdots,\mu_k)\propto\left\{\begin{aligned}&1,\parallel{x-\mu_z}\parallel^2=\min_k\parallel{x-\mu_k}\parallel^2;\\&0,\parallel{x-\mu_z}\parallel^2\gt\min_k\parallel{x-\mu_k}\parallel^2\end{aligned}\right.$
         - M步骤找到最优参数$\theta=\left\{\mu_1,\mu_2,\cdots,\mu_k\right\}$使似然函数最大$\theta=\operatorname{arg\,max}\sum\limits_{i=1}^m\sum\limits_z{Q(z)\log\frac{P(x,z|\theta)}{Q(z)}}\iff\operatorname{arg\,min}\sum\limits_{i=1}^m\parallel{x-\mu_z}\parallel^2$，即对每个簇求新的中心点$\mu_k$
   - 高斯混合模型GMM
      - 生成式模型
      - 使用EM算法迭代计算
      - 指定K值，并假设每个簇符合高斯分布
      - 模型公式$p(x)=\sum\limits_{i=1}^K\pi_iN(x|\mu_i,\sum\limits_i)$
      - 步骤
         - 随机选取每个分模型的3个参数：均值，方差和权重
         - 获得每个数据点由各个分模型分布生成的概率
         - 根据数据点和概率更新参数直至收敛
      - 优点
         - 可以给出样本属于某类的概率是多少
         - 可以用于概率密度估计
         - 可以用于生成新的样本点
   - 自组织映射神经网络SOM
      - 用途
         - 聚类
         - 高维可视化
         - 数据压缩
         - 特征提取
      - 本质
         - 输入层
         - 输出层（竞争层）
            - 神经元个数为聚类个数
      - 过程
         - 1.随机初始化权重
         - 2.竞争：判别函数值$d_j(x)=\sum\limits_{i=1}^D(x_i-w_{i,j})^2$最小的神经元获胜
         - 3.合作：更新获胜神经元临近节点参数
            - $T_{j,I(x)}(t)=\exp(-\frac{S_(j,I(x))^2}{2\sigma(t)^2})$，其中$S_{ij}$表示竞争层神经元$j$到激活结点$i$之间的距离
            - $\sigma(t)=\sigma_0\exp(-\frac{t}{\tau_\sigma})$随$t$衰减
         - 4.适应：调整相关神经元连接权重
            - $\Delta{w_{ji}}=\eta(t)\times{T_{j,I(x)}(t)}\times(x_i-w_{ji}))$
            - $\eta(t)=\eta_0\exp(-\frac{t}{\tau_\eta})$
         - 5.迭代：返回2直至特征映射趋于稳定
      - 特点
         - 保存映射
         - 保持拓扑结构
         - 交互方式：墨西哥帽
            - 近邻互相激励
            - 远邻互相抑制
            - 更远邻弱激励
         - 不需要指定类的个数
            - 聚类结果的实际簇可能小于神经元个数
         - 受noise data影响小
         - 易于可视化
         - 拓扑关系优雅
      - 实现
         - 1.设定输出层神经元个数
         - 2.设计输出层结点的排列组合
         - 3.初始化权值
            - 尽量使权值初始位置与样本的大概分布区域充分重合
            - 可以从训练集中随机抽取m个输入样本作为初始权值
         - 4.设计拓扑领域
            - 使领域不断缩小
            - 相邻神经元权向量既有区别又有一定相似性
            - 领域形状
               - 正方形
               - 六边形
               - 菱形
            - 优势领域大小用半径表示
               - 通常凭借经验
         - 5.设计学习率
            - 递减函数
- 评估指标
   - 数据簇
      - 以中心定义
         - 球形分布
      - 以密度定义
         - 稠密/稀疏
      - 以连通定义
         - 图结构
         - 适用于不规则形状或缠绕
      - 以概念定义
         - 有共性
   - 质量评估
      - 估计聚类趋势
         - 检测是否存在非随机的簇结构
            - 观察聚类误差是否随K的数量增加而单调变化
               - 随机结构变化不显著
            - 霍普金斯统计量
               - 判断数据在空间上的随机性
               - 从样本中随机找n个点$p_1,p_2,\cdots,p_n$
               - 对这n个点在样本空间中找到最近的点，并计算距离$x_1,x_2,\cdots,x_n$
               - 在样本的可能值范围内随机生成n个点$q_1,q_2,\cdots,q_n$
               - 找到最近的样本点，并计算距离$y_1,y_2,\cdots,y_n$
               - 霍普金斯统计量$H=\frac{\sum\limits_{i=1}^ny_i}{\sum\limits_{i=1}^nx_i+\sum\limits_{i=1}^ny_i}$
                  - $H$接近0.5：随机分布
                  - H接近1：聚类趋势明显
      - 判定数据簇数
         - 手肘法
         - Gap Statistic
      - 测定聚类质量
         - 簇的分离情况
         - 簇的紧凑情况
         - 常用指标
            - 轮廓系数
               - $s(p)=\frac{b(p)-a(p)}{\max\left\{a(p),b(p)\right\}}$
               - $a(p)$是点p与同簇中其他点的平均距离
               - $b(p)$是点p与最近其他簇中的平均距离
               - 求所有点$s(p)$的均值衡量聚类效果
            - 均方根标准偏差RMSSTD
               - 衡量同质性——紧凑程度
               - $RMSSTD=\left\{\frac{\sum\limits_i\sum\limits_{x\in{C_i}}\parallel{x-c_i}\parallel^2}{P\sum\limits_i(n_i)-1}\right\}^\frac{1}{2}$
               - $c_i$是第$C_i$个簇的中心
               - $n_i$是对应簇的样本量
               - $P$是样本点向量维数
            - R方
               - 衡量差异度
               - $RS=\frac{\sum\limits_{x\in{D}}\parallel{x-c}\parallel^2-\sum\limits_i\sum\limits_{x\in{c_i}}\parallel{x-c_i}\parallel^2}{\sum\limits_{x\in{D}}\parallel{x-c}\parallel^2}$
               - $c$是整个数据集的中心点
            - 改进的Hubert$\Gamma$统计
               - 通过数据对的不一致性评估聚类差异
               - $\Gamma=\frac{2}{n(n-1)}\sum\limits_{x\in{D}}\sum\limits_{y\in{D}}d(x,y)d_{x\in{C_i},y\in{C_j}}(c_i,c_j)$
               - $d(x,y)$表示点$x,y$之间的距离
               - $\Gamma$越大说明聚类结果与样本原始距离吻合，即聚类质量越高