---
{"dg-publish":true,"permalink":"/books/machine-learning//8/","tags":["百面机器学习"]}
---


# 采样

## 重采样
- 方法
   - 自助法
   - 刀切法
- 用途
   - 保证特定信息下有意识地改变样本分布
      - 样本不均衡问题
   - 随机模拟模型
      - 吉布斯采样

## 随机数
- 伪随机数
   - 线性同余
      - 区间$[0,m-1]$上的随机数$x_{t+1}=a\cdot{x_t}+c(\mod{m})$
      - 随机种子$x_0$
      - 随机数不相互独立

## 方法
- 逆变换采样
   - 从均匀分布$U(0,1)$产生一个随机数$u_i$
   - 计算$x_i=\phi^{-1}(u_i)$其中$\phi^{-1}$是累积分布函数的逆函数
   - 不适用
      - 累积分布函数的逆函数无法求解
      - 累积分布函数的逆函数不易计算
- 拒绝采样
   - 原理
      - 目标分布$p(x)$
      - 容易抽样的参考分布$q(x)$
      - 存在$\forall{p(x)\le{M\cdot{q(x)}}}$
   - 步骤
      - 1.从参考分布$q(x)$中随机抽取样本$x_i$
      - 2.从均匀分布$U(0,1)$中产生随机数$u_i$
      - 3.如果$u_i<\frac{p(x_i)}{Mq(x_i)}$，则接受样本$x_i$，否则重复1~3直到新样本被接受
- 重要性采样
   - 用于计算函数$f(x)$在目标分布$p(x)$上的函数期望，即$E[f]=\int{f(x)p(x)}dx$
   - 步骤
      - 选取一个容易抽样的参考分布$q(x)$
      - 样本$x$的重要性权重$w(x)=\frac{p(x)}{q(x)}$
      - 从参考分布$q(x)$中抽取N个样本${x_i}$
      - 估计$E[f]\approx\hat{E_N}[f]=\sum\limits_{i=1}^Nf(x_i)w(x_i)$
- 重要性重采样
   - 不需要计算函数积分
   - 从参考分布$q(x)$中抽取N个样本
   - 按重要性权重$w(x_i)$对这些样本进行重新采样
- 马尔可夫蒙特卡洛采样
   - 目的
      - 解决高维空间，拒绝采样和重要性采样难以找合适的参考分布，采样效率低的问题
   - 原理
      - 蒙特卡洛法
      - 马尔可夫链
      - 使马尔可夫链平稳的分布就是目标分布
   - 常见方法
      - Metropolis-Hastings
         - 选择一个容易采样的参考条件分布$q(x^*|x)$
         - 令$A(x,x^*)=\min{\left\{1,\frac{p(x^*)q(x|x^*)}{p(x)q(x^*|x)}\right\}}$
         - 随机选一个初始样本$x^{(0)}$
         - 对$t=1,2,3,\cdots$做如下操作
            - 根据参考条件分布$q(x^*|x^{(t-1)})$抽取样本$x^*$
            - 根据均匀分布$U(0,1)$生成随机数$u$
            - 若$u\lt{A(x^{(t-1)},x^*)}$则$x^{(t)}=x^*$，否则$x^{(t)}=x^{(t-1)}$
      - 吉布斯采样
         - Metropolis-Hastings的特例，每次只对样本的一个维度采样和更新
         - 步骤
            - 随机选择初始状态$x^{(0)}=(x_1^{(0)},x_2^{(0)},\cdots,x_d^{(0)})$
            - 对$t=1,2,3,\cdots$做如下操作
               - 对前一步样本$x^{(t-1)}=(x_1^{(t-1)},x_2^{(t-1)},\cdots,x_d^{(t-1)})$依次采样和更新每个维度的值
               - 依次抽取分量$x_i^{(t)}\sim{p}(x_1|x_1^{(t)},x_2^{(t)},\cdots,x_{i-1}^{(t)},x_{i+1}^{(t-1)},\cdots,x_d^{(t-1)})$
               - 形成新的样本$x^{(t)}=(x_1^{(t)},x_2^{(t)},\cdots,x_d^{(t)})$
      - 如何得到独立样本
         - 相邻样本不独立
         - 方法
            - 多条马尔可夫链
            - 同一马尔可夫链每隔若干条取一个样本

## 高斯分布
- 逆变换采样
   - 在均匀分布$[0,1]$上产生随机数$u$
   - 令$z=\sqrt{2}erf^{-1}(2u-1)$，则$z$服从标准正态分布
   - $erf(x)=\frac{2}{\sqrt{\pi}}\int_0^xe^{-t^2}dt$
   - $erf^{-1}$没有显示解
   - 改用两个独立的高斯联合分布求解
      - Box-Muller算法
         - 原理
            - 两个独立高斯分布的联合概率密度$p(x,y)=\frac{1}{2\pi}e^{-\frac{x^2+y^2}{2}}$
            - 累积分布函数$F(R)=\int_{x^2+y^2\le{R^2}}\frac{1}{2\pi}e^{\frac{x^2+y^2}{2}}dxdy$
            - 转化为极坐标求二重积分得$F(R)=1-e^{\frac{R^2}{2}}$，逆函数容易求解
         - 步骤
            - 在$[0,1]$上产生两个独立的均匀分布随机数$u_1,u_2$
            - 可以产生两个相互独立的标准正态分布
               - $x=\sqrt{-\ln(u_1)}\cos{2\pi}u_2$
               - $y=\sqrt{-\ln(u_1)}\sin{2\pi}u_2$
      - Marsaglia polar method
         - 避开三角函数计算，降低计算耗时
         - 步骤
            - 在矩形$\left\{(x,y)|-1\le{x},y\le{1}\right\}$上利用拒绝采样法在单位圆盘$\left\{(x,y)|x^2+y^2\le{1}\right\}$上产生均匀分布随机数对$(x,y)$
            - 令$s=x^2+y^2$则产生两个独立的标准正态分布样本
               - $x\sqrt\frac{-2\ln{s}}{s}$
               - $y\sqrt\frac{-2\ln{s}}{s}$
- 拒绝采样
   - 选取指数分布作为参考分布
      - 原理
         - 取$\lambda=1$的指数分布，密度函数为$q(x)=e^{-x}$
         - 累积分布函数为$F(x)=1-e^{-x}$
         - 累积分布逆函数$F^{-1}(u)=-\log(1-u)$
         - 接受概率为$A(x)\frac{p(x)}{M\cdot{q(x)}}=e^{-\frac{(x-1)^2}{2}}$
         - 其中$p(x)=\frac{2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$是标准正态分布压缩到正半轴的概率密度函数
      - 步骤
         - 1.从$[0,1]$上产生均匀分布随机数$u_0$
         - 2.计算$x=F^{-1}(u_0)$得到指数分布样本$x$
         - 3.从$[0,1]$上产生均匀分布随机数$u_1$，若$u_1<A(x)$则接受$x$，否则返回1重新采样
         - 从$[0,1]$上产生均匀分布随机数$u_2$，若$u_2\lt0.5$，$x$将变为$-x$，否则保持不变
   - Ziggurat算法
      - 原理
         - 采用多个矩形逼近目标分布

## 贝叶斯网络
- 祖先采样
   - 没有观测变量
   - 有向图顺序

## 样本不均衡
- 过采样
   - SMOTE
      - 对小样本集$S_{min}$中每个样本$x$
      - 从$S_{min}$中$K$近邻中随机选取样本$y$
      - 在$x,y$连线上随机选一点作为新合成的样本
   - Borderline-SMOTE
      - 只给处在分类边界上的少类样本合成样本
   - ADASYN
      - 每个样本合成新样本个数不同
   - 数据清理
      - Tomek Links
   - 数据扩充
      - 增加噪声扰动
      - 基础变换
- 欠采样
   - Informed undersampling
      - 目的
         - 解决随机性带来的数据丢失问题
      - 常见算法
         - easy ensemble（类似bagging)
            - 多次抽取子集
            - 训练多个分类器
            - 模型融合
         - balance cascade（类似boost）
            - 级联结构
            - 从每级中随机抽取子集
            - 训练分类器
            - 剔除原数据集中能被当前分类器判别的样本
            - 继续下一级操作
         - NearMiss
            - 利用K近邻挑选具有代表性的样本
   - 数据清理
      - one sided selection
   - hard negative mining
- 算法
   - 改变目标函数
      - 代价敏感学习
         - 不同类别权重不同
   - 改变问题
      - 单类学习
      - 异常检测