---
{"dg-publish":true,"permalink":"/books/machine-learning//6/","tags":["百面机器学习"]}
---


# 概率图模型

## 结构
- 节点
   - 随机变量
   - 类型
      - 观测节点
         - 观测的数据
      - 隐含节点
         - 潜在的知识
- 边
   - 随机变量的依赖或相关关系
   - 类型
      - 有向边
      - 无向边

## 模型
- 贝叶斯网络
   - 有向图
- 马尔科夫网络
   - 无向图
   - 团：图中任意两点之间有边的子集
   - 联合概率密度
      - $P(x)=\frac{1}{Z}\prod_{Q\in{C}}\varphi_Q(x_Q)$
      - $Z=\sum\limits_x\prod_{Q\in{C}}\varphi_Q(x_Q)$
      - $C$为最大团构成的集合
      - $\varphi_Q$为团$Q$对应的势函数，非负，且在概率较大时取值大，可以用指数函数如$\varphi_Q(x_Q)=e^{-H_Q(x_Q)}$
      - $H_Q(Q_x)=\sum\limits_{u,v\in{Q},u\not=v}\alpha_{u,v}x_ux_v+\sum\limits_{v\in{Q}}\beta_vx_v$
- 隐马尔科夫模型
   - 无后效性的随机过程
   - 参数
      - 隐状态间的转移概率
      - 隐状态到观测状态的输出概率
      - 隐状态的取值空间
      - 观测状态的取值空间
      - 初始状态的概率分布
   - 基本问题
      - 概率计算
         - 已知模型参数
         - 计算观测序列出现的概率Y
         - 算法
            - 前向
            - 后向
      - 预测
         - 已知模型参数和观测序列Y
         - 计算最可能的隐状态序列X
         - 算法
            - 动态规划
               - 维特比算法
      - 学习
         - 已知观测序列Y
         - 求该观测序列概率最大的模型参数
         - 算法
            - Baum-Welch(一种EM算法)
   - 应用
      - 序列标注
- 最大熵模型
   - 熵：不确定的度量
   - $H(P)=-\sum\limits_xP(x)\log{P(x)}$
   - 当$x$服从均匀分布时熵最大
   - 条件熵$H(P)=-\sum\limits_{x,y}\widetilde{P}(x)P(y|x)\log{P(y|x)}$，其中$\widetilde{P}(x)$是样本在训练集上的经验分布
   - 约束$E_{\widetilde{P}}(f)=E_P(f)$
      - 特征函数$f(x,y)$关于经验分布$\widetilde{P}(x,y)$的期望值是$E_{\widetilde{P}}(f)=\sum\limits_{x,y}\widetilde{P}(x,y)f(x,y)$
      - $f(x,y)$关于模型$P(y|x)$和经验分布$\widetilde{P}(x)$的期望为$E_P(f)=\sum\limits_{x,y}\widetilde{P}(x)P(y|x)f(x,y)$
   - 最优化问题
      - $\left\{\begin{aligned}&\max_PH(P)=-\sum\limits_{x,y}\widetilde{P}(x)P(y|x)\log{P}(y|x),\\&s.t.,E_{\widetilde{P}}(f_i)=E_P(f_i),\forall{i}=1,2,\cdots,M,\\&\sum\limits_yP(y|x)=1\end{aligned}\right.$
   - 求解后的模型表达式$P_w(y|x)=\frac{1}{Z}\exp(\sum\limits_{i=1}^Mw_if_i(x,y))$
- 最大熵马尔可夫模型MEMM
   - 去除隐马尔可夫模型中观测状态相互独立的假设
   - 模型
      - $p(x_{1\cdots{n}}|y_{1\cdots{n}})=\prod_{i=1}^np(x_i|x_{i-1},y_{1\cdots{n}})$
      - $p(x_i|x_{i-1},y_{1\cdots{n}})=\frac{\exp(F(x_i,x_{i-1},y_{1\cdots{n}}))}{Z(x_{i-1},y_{1\cdots{n}})}$
      - $Z(x_{i-1},y_{1\cdots{n}})=\sum\limits_{x_i}\exp(F(x_i,x_{i-1},y_{1\cdots{n}}))$
      - $F(x_i,x_{i-1},y_{1\cdots{n}})$为$x_i,x_{i-1},y_{1\cdots{n}}$所有特征的线性叠加
   - 标注偏置问题
      - 隐状态倾向于向后续状态更少的状态上转移，以提高整体的后验概率
- 条件随机场CRF
   - 在最大熵马尔可夫模型的基础上进行了全局归一化
   - 模型$p(x_{1\cdots{n}}|y_{1\cdots{n}})=\frac{1}{Z(y_{1\cdots{n}})}\prod_{i=1}^n\exp(F(x_i,x_{i-1},y_{1\cdots{n}}))$
   - 解决了局部归一化带来的标注偏置问题
- 主题模型
   - 解决N-gram模型无法识别不同词具有的相同主题
   - pLSA
      - 给定文章生成词的概率为$p(w|d)=\sum\limits_zp(w|z,d)p(z|d)=\sum\limits_zp(w|z)p(z|d)$
      - $L=\prod_m^M\prod_n^Np(d_m,w_n)^{c(d_m,w_n)}$
      - 使用EM算法求解
      - 频率派
   - LDA
      - pLSA的贝叶斯版本
      - 主题分布和词分布加入Dirichlet先验
         - Dirichlet分布式是多项式分布的共轭先验概率分布
         - 后验分布仍然服从Dirichlet分布
         - 便于计算
      - 贝叶斯派
      - 步骤
         - 从超参数为$\alpha$的Dirichlet分布中抽样生成文档$d_i$的主题分布$\theta_i$
         - 对文档$d_i$中的每个词操作
            - 根据多项式分布$\theta_i$生成主题$z_{ij}$
            - 从超参数为$\beta$的Dirichlet分布中抽样生成主题$z_{ij}$对应的词分布$\psi_{zij}$
            - 从多项式分布$\psi_{zij}$中抽样生成词$w_{ij}$
         - 使用吉布斯采样求解$\theta_i$和$\psi_{zij}$的期望
            - 随机给定每个单词新主题
            - 单词的转移概率为：给定文章中所有单词以及除自身以外其他所有单词的主题，在此条件下改词对应各个新主题的概率
            - 多次迭代后收敛
      - 设定主题个数K
         - 验证集检验超参数效果
         - 评估指标
            - 困惑度
            - $perplexity(D)=\exp(-\frac{\sum\limits_{d=1}^M\log{p(w_d)}}{\sum\limits_{d=1}^MN_d})$
   - 非参数主题模型HDP-LDA
      - 在LDA基础上融入分层Dirichlet过程HDP
      - 不需要预先指定主题个数K
      - 缺点
         - 概率题模型更加复杂
         - 训练速度缓慢
   - 应用
      - 冷启动
         - 类型
            - 用户
               - 新用户
            - 物品
               - 新item
            - 系统
               - 新推荐系统
         - 方法
            - 基于内容推荐
               - 用户
                  - 推测用户兴趣主题
                     - 注册信息
                     - 搜索关键词
                     - 站外信息
                     - 历史行为
               - item
                  - 推测item主题
                     - 类别
                     - 关键词
                     - 基础信息
               - 系统
                  - 用户主题
                  - item主题
                  - 用户-item先验关系

## 优点
- 使用简洁的图示表示概率生成关系

## 类型
- 生成式
   - 联合概率分布
      - $P(Y|X) = \frac{P(X,Y)}{P(X)}=\frac{\sum\limits_ZP(X,Y,Z)}{\sum\limits_{Y,Z}P(X,Y,Z)}$
   - 朴素贝叶斯
   - 贝叶斯网络
   - pLSA
   - LDA
   - 隐马尔科夫
- 判别式
   - 条件概率分布
      - $P(Y|X)=\sum\limits_ZP(Y,Z|X)$
   - 最大熵模型
   - 条件随机场
   - 最大熵隐马尔可夫