---
{"dg-publish":true,"permalink":"/books/machine-learning//1/","tags":["百面机器学习"]}
---


# 特征工程

## 1. 特征归一化
- 目的：消除特征之间量纲的影响，使得不同特征之间具有可比性
- 实现：
   - 0-1: $X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$
   - z-score: $z=\frac{x-\mu}{\sigma}$
- 一般范围：
   - 通过梯度下降法求解的模型
   - 基于欧式距离

## 2. 类别特征
- 不需要处理的模型：树模型
- 转换成数值型特征方法：
   - 序号编码：类别有大小关系
   - one-hot：
      - 节省空间：向量稀疏表示v=[0,0,0,0,1,0,3,0,0,0] -> (10,[4,6],[1,3])
      - 降维
         - 特征选择
         - 高维->维度诅咒
            - 欧式距离失效
            - 模型过拟合
   - 二进制编码
      - 方法：使用编码的二进制表示
      - 本质：哈希映射
      - 优点：维度少于one-hot
   - 其他：如Helmert Contrast、Sum Contrast、Polynomial Contrast、Backward Difference Contrast等

## 3. 组合特征
- 如何：一阶离散特征两两组合
- id类组合特征需要降维：
   - MF-矩阵分解：M * N -> M * k * k * N
- 如何有效找到组合特征：
   - 决策树
      - 梯度提升决策树
      - 基于残差

## 4. 文本表示模型
- 词袋模型(bag of words)
   - 词袋模型忽略词在文章中的出现顺序
   - 每篇文章用一个长向量表示，向量中的每一维表示一个单词的重要程度，常用TF-IDF表示
   - TF-IDF
      - TF(t,d)：单词t在文档d中出现频率
      - IDF(t)：$log\frac{文章总数}{包含单词t的文章总数+1}$
   - N-gram
      - 词干抽取word stemming
- 主题模型(topic model)-从文本库中发现有代表性的主题
   - 基于概率图模型的生成式模型
   - LDA
      - 文档中单词共现关系
      - 对文档-单词进行矩阵分解得到文档-主题，主题-单词的概率分布
      - LDA主题模型 vs LDA判别分析
         - 主题模型：Latent Dirichlet Allocation
         - 判别分析：Linear Discriminant Analysis
   - LSA（潜在语义分析）
- 词嵌入模型(word embedding)
   - 核心思想：将每个词映射成低维空间(K=50~300)的一个稠密向量(dense vector)，每一维可以看作一个隐含主题
   - 有N个词的文章可以看成N * K的矩阵，在浅层机器学习中直接使用这种矩阵特征，通常效果不好，往往需要特征工程提升效果，而深度学习模型提供了一种自动进行特征工程的方式。
   - word2vec
      - cbow: 根据上下文预测当前词的生成概率
      - skip-gram: 根据当前词预测上下文中各词的生成概率
      - 可以看成是对“上下文-单词”进行矩阵分解

## 5. 数据不足时的处理方法
- 模型
   - 简化模型
   - 增加罚项(L1/L2)
   - 集成学习
   - dropout超参数
- 数据
   - 图像的旋转、平移、缩放、裁剪、填充、左右翻转等
   - 图像中加入噪声扰动
   - 改变图像颜色、亮度、清晰度、对比度、锐度
   - 上采样：SMOTE算法
   - 生成式对抗网络模型
   - 迁移学习