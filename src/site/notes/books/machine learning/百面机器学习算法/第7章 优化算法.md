---
{"dg-publish":true,"permalink":"/books/machine-learning//7/","tags":["百面机器学习"]}
---


# 优化算法

## 损失函数
- 分类
   - 0-1损失函数
   - hinge损失函数
      - 0-1损失函数的凸上界
      - $L_{hinge}(f,y)=\max(0,1-fy)$
   - logistic损失函数
      - 0-1损失函数的凸上界
      - 处处可微
      - 异常值敏感
      - $L_{logistic}(f,y)=\log_2(1+\exp(-fy))$
   - 交叉熵损失函数
      - $L_{cross_entropy}(f,y)=-\log_2\frac{1+fy}{2}$
      - ![Pasted image 20230324180835.png](/img/user/Pasted%20image%2020230324180835.png)
- 回归
   - 平方损失
      - 异常点敏感
   - 绝对损失
      - 中值回归
      - 对异常点更鲁棒
   - huber损失
      - $L_{Huber}(f,y)=\left\{\begin{aligned}&(f-y)^2,\left|f-y\right|\le\delta\\&2\delta|f-y|-\delta^2,|f-y|\gt\delta\end{aligned}\right.$
   - ![Pasted image 20230324181455.png](/img/user/Pasted%20image%2020230324181455.png)

## 优化类型
- 非凸优化
   - 主成分分析
      - $min_{VV^T=I_k}L(V)=\parallel{X-V^TVX}\parallel_F^2$
      - $\begin{aligned}L(\frac{1}{2}V+\frac{1}{2}(-V))&=L(0)\\&=\parallel{X}\parallel_F^2\\&\gt\parallel{X-V^TVX}\parallel_F^2\\&=\frac{1}{2}L(V)+\frac{1}{2}L(-V)\end{aligned}$
   - 低秩模型
      - 矩阵分解
   - 深度神经网络
- 凸优化
   - 逻辑回归
      - 凸性：二阶Hessian矩阵半正定
   - SVM
   - 线性回归

## 算法
- 直接法
   - 条件
      - 损失函数是凸函数
      - 一阶偏导=0有闭式解
   - 应用
      - 岭回归
         - 目标函数$L(\theta)=\parallel{X\theta-y}\parallel_2^2+\lambda\parallel{\theta}\parallel_2^2$
         - 解析解$\theta^*=(X^TX+\lambda{I})^{-1}X^Ty$
- 迭代法
   - 目标
      - $\delta_t=\underset{\delta}{\operatorname{arg\,min}}L(\theta_t+\delta)$
      - $\theta_{t+1}=\theta_t+\delta_t$
   - 一阶法
      - 对$L(\theta_t+\delta)$做一阶泰勒展开
      - 得到近似式$L(\theta_t+\delta)\approx{L(\theta_t)+\nabla{L(\theta_t)}}\delta$
      - 加上$L_2$正则项后的最优化问题$\delta_t=\underset\delta{\operatorname{arg\,min}}(L(\theta_t)+\nabla{L}(\theta_t)^T\delta+\frac{1}{2\alpha}\parallel\delta\parallel_2^2)=-\alpha\nabla{L}(\theta_t)$
      - 迭代公式$\theta_{t+1}=\theta_t-\alpha\nabla{L}(\theta_t)$
   - 二阶法
      - 对$L(\theta_t+\delta)$做二阶泰勒展开
      - 得到近似式$L(\theta_t+\delta)\approx{L(\theta_t)}+\nabla{L}(\theta_t)^T\delta+\frac{1}{2}\delta^T\nabla^2L(\theta_t)\delta$
      - 最优化问题为$\delta_t=\underset\delta{\operatorname{arg\,min}}(L(\theta_t)+\nabla{L}(\theta_t)^T\delta+\frac{1}{2}\delta^T\nabla^2L(\theta_t)\delta)=-\nabla^2L(\theta_t)^{-1}\nabla{L(\theta_t)}$
      - 迭代公式$\theta_{t+1}=\theta_t-\nabla^2L(\theta_t)^{-1}\nabla{L}(\theta_t)$
      - 收敛速度远大于一阶法
      - 高维情况下Hessian矩阵计算复杂度大
      - 当目标函数非凸时，可能会收敛到鞍点
   - 梯度下降法
      - 经典梯度下降
         - 所有训练数据
      - 随机梯度下降法
         - 单个训练样本近似
         - 影响较大的异常点
            - 山谷
               - 震荡
            - 鞍点
      - mini-batch梯度下降
         - 选取参数m
            - 一般取2的幂次能充分利用矩阵运算
         - 如何选m个训练数据
            - 全局随机排序
            - 依次选取m个数据
      - 动量法
         - 动量公式$v_t=\gamma{v_{t-1}}+\eta{g_t}$
         - 参数更新公式$\theta_{t+1}=\theta_t-v_t$
      - AdaGrad法
         - 环境感知：自适应确定参数的学习率
         - 更新频率低的参数更新步幅大，频率高的参数步幅小
         - 参数更新公式$\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{\sum\limits_{k=0}^tg_{k,i}^2+\epsilon}}g_{t,i}$
      - Adam方法
         - 结合动量法和AdaGrad
         - 一阶矩
            - 动量
            - $m_t=\beta_1m_{t-1}+(1-\beta_1)g_t$
         - 二阶矩
            - 环境感知
            - $v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2$
         - 指数衰退平均
         - 参数更新公式$\theta_{t+1}=\theta_t-\frac{\eta\cdot\hat{m_t}}{\sqrt{\hat{v_t}+\epsilon}}$
         - $\hat{m_t}=\frac{m_t}{1-\beta_1^t}$
         - $\hat{v_t}=\frac{v_t}{1-\beta_2^t}$
      - Nesterov Accelerated Gradient
         - 计算未来可能位置处的梯度，而非当前位置梯度
      - AdaDelta
      - RMSProp
         - AdaGrad的变种
         - 指数衰退平均代替求和
      - AdaMax
         - Adam的变种
         - 梯度平方改为指数衰退求最大
      - Nadam
         - NAG版的adam

## 参数稀疏性
- 优点
   - 防止过拟合
   - 提高泛化能力
- L1正则使模型参数具有稀疏性
   - 解空间形状
   - 函数叠加
   - 贝叶斯先验