---
{"dg-publish":true,"permalink":"/books/machine-learning///","tags":["百面机器学习"]}
---

- [ ] $\star$ 1. 为什么需要对数值类型的特征做归一化？
- [ ] $\star\star$ 2. 怎样处理类别特征？
- [ ] $\star\star$ 3. 什么是组合特征？如何处理高维组合特征？
- [ ] $\star\star$ 4. 怎样有效地找到组合特征？
- [ ] $\star\star$ 5. 有哪些文本表示模型？它们各自有什么优缺点？
- [ ] $\star\star$ 6. 如何缓解图像分类任务中训练数据不足带来的问题？
- [ ] $\star\star\star$ 7. word2vec是如何工作的？它和隐狄利克雷模型有什么区别和联系？
- [ ] $\star$ 8. 准确性的局限性
- [ ] $\star$ 9. 精确率与召回率的权衡
- [ ] $\star$ 10. 平方根误差的意外
- [ ] $\star$ 11. 什么是ROC曲线
- [ ] $\star$ 12. 为什么要进行在线A/B测试
- [ ] $\star$ 13. 如何进行线上A/B测试
- [ ] $\star$ 14. 过拟合和欠拟合具体是什么现象
- [ ] $\star\star$ 15. 如何绘制ROC曲线
- [ ] $\star\star$ 16. 如何计算AUC
- [ ] $\star\star$ 17. 为什么在一些场景中使用余弦相似度而不是欧式距离
- [ ] $\star\star$ 18. 如何划分实验组和对照组
- [ ] $\star\star$ 19. 模型评估过程中的验证方法及其优缺点
- [ ] $\star\star$ 20. 能否说出集中降低过拟合和欠拟合风险的方法
- [ ] $\star\star\star$ 21. ROC曲线相比P-R曲线有什么特点
- [ ] $\star\star\star$ 22. 余弦距离是否是一个严格定义的距离
- [ ] $\star\star\star$ 23. 自助采样在极限情况下会有多少数据从未被选择过
- [ ] $\star\star\star$ 24. 超参数有哪些调优方法
- [ ] $\star\star$ 25. 逻辑回归相比线性回归，有何异同
- [ ] $\star\star$ 26. 决策树有哪些常用的启发式函数
- [ ] $\star\star\star$ 27. 线性可分的两类点在SVM分类超平面上的投影仍然线性可分吗
- [ ] $\star\star\star$ 28. 证明存在一组参数使得高斯核svm的训练误差为0
- [ ] $\star\star\star$ 29. 加入松弛变量的SVM的训练误差可以为0吗
- [ ] $\star\star\star$ 30. 用逻辑回归处理多标签分类任务的一些相关问题
- [ ] $\star\star\star$ 31. 如何对决策树进行剪枝
- [ ] $\star\star\star\star$ 32. 训练误差为0的SVM分类器一定存在吗
- [ ] $\star\star$ 33. 从最大方差的角度定义PCA的目标函数并给出求解方法
- [ ] $\star\star$ 34. 从回归的角度定义PCA的目标函数并给出对应的求解方法
- [ ] $\star\star$ 35. 线性判别分析的目标函数以及求解方法
- [ ] $\star\star$ 36. 线性判别分析与主成分分析的区别于联系
- [ ] $\star\star$ 37. K均值聚类算法的步骤是什么
- [ ] $\star\star$ 38. 高斯混合模型的核心思想是什么？它是如何迭代计算的
- [ ] $\star\star\star$ 39. K均值聚类的优缺点是什么？如何进行调优
- [ ] $\star\star\star$ 40. 针对K均值聚类的缺点，有哪些改进的模型
- [ ] $\star\star\star$ 41. 自组织神经网络是如何工作的？它与K均值算法有何区别
- [ ] $\star\star\star$ 42. 怎样设计自组织神经网络并设定网络训练参数
- [ ] $\star\star\star$ 43. 以聚类算法为例，如何区分两个非监督学习算法的优劣
- [ ] $\star\star\star\star$ 44. 证明K均值聚类算法的收敛性
- [ ] $\star$ 45. 写出图6.1(a)中贝叶斯网络的联合概率分布
- [ ] $\star$ 46. 写出图6.1(b)中马尔科夫网络的联合概率分布
- [ ] $\star\star$ 47. 解释贝叶斯朴素模型的原理，并给出概率图模型表示
- [ ] $\star\star$ 48. 解释最大熵模型的原理，并给出概率图模型表示
- [ ] $\star\star$ 49. 常见的主题模型有哪些？试介绍其原理
- [ ] $\star\star$ 50. 如何确定LDA模型中的主题个数
- [ ] $\star\star\star$ 51. 常见的概率图模型中，哪些是生成式的，哪些是判别式的
- [ ] $\star\star\star$ 52. 如何对中文分词问题用隐马尔科夫模型进行建模和训练
- [ ] $\star\star\star$ 53. 如何用主题模型解决推荐系统中的冷启动问题
- [ ] $\star\star\star\star$ 54. 最大熵马尔科夫模型为什么会产生标注偏置问题？如何解决
- [ ] $\star$ 55. 有监督学习涉及的损失函数有哪些
- [ ] $\star$ 56. 训练数据量特别大时经典梯度法存在的问题，如何改进
- [ ] $\star\star$ 57. 机器学习中那些是凸优化问题？哪些是非凸优化问题？
- [ ] $\star\star$ 58. 无约束优化问题的求解
- [ ] $\star\star$ 59. 随机梯度下降法失效的原因
- [ ] $\star\star\star$ 60. 如何验证求目标函数梯度功能的正确性
- [ ] $\star\star\star$ 61. 随机梯度下降法的一些变种
- [ ] $\star\star\star$ 62. L1正则化使得模型参数具有稀缺性的原理是什么
- [ ] $\star$ 63. 如何编程实现均匀分布随机数生成器
- [ ] $\star$ 64. 简述MCMC采样的主要思想
- [ ] $\star\star$ 65. 举例说明采样在机器学习中的应用
- [ ] $\star\star$ 66. 简单介绍几种常见的MCMC采样法
- [ ] $\star\star$ 67. MCMC采样法如何得到项目独立的样本
- [ ] $\star\star\star$ 68. 简述一些常见的采样方法的主要思想和具体操作
- [ ] $\star\star\star$ 69. 如何对高斯分布进行采样
- [ ] $\star\star\star$ 70. 如何对贝叶斯网络进行采样
- [ ] $\star\star\star$ 71. 当训练集中正负样本不均衡时，如何处理数据以更好的训练分类模型
- [ ] $\star$ 72. 写出常用激活函数以及其导数
- [ ] $\star$ 73. 神经网络训练时是否可以将参数全部初始化为0
- [ ] $\star\star$ 74. 多层感知机表示异或逻辑时最少需要几个隐层
- [ ] $\star\star$ 75. 为什么Sigmoid和Tanh激活函数会导致梯度消失的现象
- [ ] $\star\star$ 76. 写出多层感知机的平方误差和交叉损失函数
- [ ] $\star\star$ 77. 解释卷积操作中的洗漱交互和参数共享以及其作用
- [ ] $\star\star\star$ 78. 一个隐层需要多少隐节点能够实现包含n元输入的任意布尔函数
- [ ] $\star\star\star$ 79. 多个隐层实现包含n元输入的任意布尔函数最少需要多少个节点和网络层
- [ ] $\star\star\star$ 80. ReLU系列的激活函数的优点是什么？他们有什么局限性以及如何改进？
- [ ] $\star\star\star$ 81. 平方误差损失函数和交叉熵损失函数分别适合什么场景
- [ ] $\star\star\star$ 82. 为什么Dropout可以抑制过拟合？简述它的工作原理和实现
- [ ] $\star\star\star$ 83. 批量归一化的基本动机和原理是什么？在卷积神经网络中如何使用
- [ ] $\star\star\star$ 84.  常用的池化操作有哪些？池化作用是什么
- [ ] $\star\star\star$ 85. 卷积神经网络如何用于文本分类任务
- [ ] $\star\star\star$ 86. ResNet的提出背景和核心理论是什么
- [ ] $\star\star\star\star$ 87. 根据损失函数推导各层参数更新的梯度计算公式
- [ ] $\star$ 88. 循环神经网络与前馈神经网络相比有什么特点
- [ ] $\star\star$ 89. 循环神经网络为什么会出现梯度消失或梯度爆炸？有哪些改进方案
- [ ] $\star\star$ 90. LSTM是如何实现长短期记忆功能的
- [ ] $\star\star$ 91. 什么是Seq2Seq模型？它有哪些优点
- [ ] $\star\star\star$ 92. 在循环神经网络中能否使用ReLU作为激活函数
- [ ] $\star\star\star$ 93. Seq2Seq模型在解码时有哪些常用方法
- [ ] $\star\star\star$ 94. Seq2Seq模型引入注意力机制是为了解决什么问题？为什么选用双向神经网络模型
- [ ] $\star$ 95. 强化学习中有哪些基本概念
- [ ] $\star\star$ 96. 从价值迭代来考虑，如何找到图中马里奥的一条最优路线
- [ ] $\star\star$ 97. 从策略迭代来考虑，如何找到图中马里奥的一条最优路线
- [ ] $\star\star\star$ 98. 什么是深度强化学习？它与传统的强化学习有什么不同？
- [ ] $\star\star\star$ 99. 在智能体与环境的交互中，什么是探索和利用？如何平衡探索与利用？
- [ ] $\star\star\star\star$ 100. 什么是策略梯度下降？与传统Q-learning有什么不同？有什么优势
- [ ] $\star$ 101. 集成学习分哪几种？它们有何异同？
- [ ] $\star$ 102. 常用的基分类器是什么？
- [ ] $\star\star$ 103. 集成学习有哪些基本步骤？请举一个集成学习的例子
- [ ] $\star\star$ 104. 可否将随机森林中的基分类器由决策树替换为线性分类器或K-近邻
- [ ] $\star\star$ 105. 什么是偏差和方差
- [ ] $\star\star$ 106. GBDT的基本原理是什么
- [ ] $\star\star$ 107. 梯度提升和梯度下降的区别和联系是什么
- [ ] $\star\star$ 108. GBDT的优点和局限性有哪些
- [ ] $\star\star\star$ 109. 如何从减小方差和偏差的角度解释Boosting和bagging的原理
- [ ] $\star\star\star$ 110. XGBoost与GBDT的联系和区别有哪些
- [ ] $\star$ 111. 简述GAN的基本思想和训练过程
- [ ] $\star\star$ 112. GANs如何避开大量概率推断计算
- [ ] $\star\star$ 113. 如何构建一个生成器，生成一串文字组成的序列来代表一个句子
- [ ] $\star\star\star$ 114. GANs的值函数
- [ ] $\star\star\star$ 115. 原GANs中存在哪些问题会成为制约模型训练效果的瓶颈
- [ ] $\star\star\star$ 116. 在生成器和判别器中应该怎样设计深层卷积结构
- [ ] $\star\star\star$ 117. 如何把一个生成网络和一个推断网络融合在GANs框架下
- [ ] $\star\star\star\star$ 118. GANs最小化目标函数过程中会遇到的问题
- [ ] $\star\star\star\star$ 119. WGAN针对前面的问题做了哪些改进？什么事Wasserstein距离
- [ ] $\star\star\star\star\star$ 120. 怎样具体应用Wasserstein距离实现WGAN算法
- [ ] $\star\star\star\star\star$ 121. 设计一种制造负样本的生成器来采样一些迷惑性强的负样本
- [ ] $\star\star\star\star\star$ 122. 训练一个序列生成器的优化目标通常是什么？GANs框架下这个优化目标有何不同
- [ ] $\star\star\star\star\star$ 123. 有了GANs下生成器的优化目标，怎样求解目标函数对生成器参数的梯度